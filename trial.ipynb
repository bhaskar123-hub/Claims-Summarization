{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e277a26",
   "metadata": {},
   "source": [
    "# Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e2eb9",
   "metadata": {},
   "source": [
    "## Create Table in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pgvector extension enabled\n",
      "Table 'claim_notes' is created successfully in PostgreSQL + pgvector\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"claims_db\",   # your DB name\n",
    "    user=\"postgres\",      # DB user\n",
    "    password=\"admin\",     # DB password\n",
    "    host=\"localhost\",     # local Docker setup\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 1Enable pgvector extension (required for vector data type)\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "print(\"✅ pgvector extension enabled\")\n",
    "\n",
    "# Create claim_notes table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS claim_notes (\n",
    "    claim_id TEXT NOT NULL,        -- ID of the claim\n",
    "    note_id TEXT NOT NULL,         -- ID of the note within claim\n",
    "    file_path TEXT,                -- path to original file for traceability\n",
    "    note_text TEXT NOT NULL,       -- text content of note\n",
    "    embedding vector(384),         -- vector embedding (dimension for MiniLM)\n",
    "    PRIMARY KEY (claim_id, note_id)  -- allows multiple notes per claim\n",
    ");\n",
    "\n",
    "-- Index on claim_id for fast retrieval\n",
    "CREATE INDEX IF NOT EXISTS idx_claim_id ON claim_notes(claim_id);\n",
    "\"\"\")\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Table 'claim_notes' is created successfully in PostgreSQL + pgvector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff606c",
   "metadata": {},
   "source": [
    "## Load and Parse Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64377c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Directory containing your claim note files\n",
    "directory = Path(\"claim_notes_txt\")\n",
    "\n",
    "notes = []\n",
    "\n",
    "# Use search instead of match for more flexibility\n",
    "pattern = re.compile(r\"claim_(\\d+)_note_(\\d+)\", re.IGNORECASE)\n",
    "\n",
    "for file in directory.glob(\"*.txt\"):\n",
    "    stem = file.stem.strip()  # remove any accidental whitespace\n",
    "    match = pattern.search(stem)\n",
    "    if match:\n",
    "        claim_id, note_id = match.groups()\n",
    "        text = file.read_text(encoding=\"utf-8\").strip()\n",
    "        notes.append({\n",
    "            \"claim_id\": claim_id,\n",
    "            \"note_id\": note_id,\n",
    "            \"file_path\": str(file),\n",
    "            \"text\": text\n",
    "        })\n",
    "    else:\n",
    "        print(f\"⚠️ Skipped file (pattern mismatch): {file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaed8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim ID: 2001, Note ID: 3001, File: claim_notes_txt\\claim_2001_note_3001.txt\n",
      "Text snippet: Called the claimant at 708-1234 to confirm the initial loss details and timeline. The claimant provi...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3002, File: claim_notes_txt\\claim_2001_note_3002.txt\n",
      "Text snippet: Spoke with the claimant at 708-1234 regarding the extent of visible damages and locations affected. ...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3003, File: claim_notes_txt\\claim_2001_note_3003.txt\n",
      "Text snippet: Completed an initial file review after receiving the claimant narrative and preliminary documents. W...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3004, File: claim_notes_txt\\claim_2001_note_3004.txt\n",
      "Text snippet: To-do: confirm receipt of photographic evidence and vendor estimate. I contacted the claimant at 708...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3005, File: claim_notes_txt\\claim_2001_note_3005.txt\n",
      "Text snippet: Called the claimant at 708-1234 to discuss coverage specifics and deductible implications. The claim...\n",
      "\n",
      "Total notes loaded: 50\n"
     ]
    }
   ],
   "source": [
    "# Display first 5 notes to verify\n",
    "for n in notes[:5]:\n",
    "    print(f\"Claim ID: {n['claim_id']}, Note ID: {n['note_id']}, File: {n['file_path']}\")\n",
    "    print(f\"Text snippet: {n['text'][:100]}...\\n\")\n",
    "\n",
    "print(f\"Total notes loaded: {len(notes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24a25a",
   "metadata": {},
   "source": [
    "## Generate Embeddings in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a616c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Claims-Summarization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Claims-Summarization\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VAIBHAVI SHARMA\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract all note texts\n",
    "texts = [note[\"text\"] for note in notes]\n",
    "\n",
    "# Generate embeddings in batch\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=16,           # adjust batch size if needed\n",
    "    convert_to_numpy=True,   # returns numpy array\n",
    "    normalize_embeddings=True  # ensures cosine similarity works well\n",
    ")\n",
    "\n",
    "# Attach embeddings back to notes\n",
    "for i, note in enumerate(notes):\n",
    "    note[\"embedding\"] = embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce825dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim ID: 2001, Note ID: 3001\n",
      "Embedding vector length: 384\n",
      "Embedding vector: [-0.10120149  0.1021888   0.09747697  0.01523357  0.06224263]...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3002\n",
      "Embedding vector length: 384\n",
      "Embedding vector: [-0.03196564  0.09648274  0.15036862  0.05041745  0.08159971]...\n",
      "\n",
      "Claim ID: 2001, Note ID: 3003\n",
      "Embedding vector length: 384\n",
      "Embedding vector: [-0.05104477  0.12402744  0.0164618   0.01731722  0.06524318]...\n",
      "\n",
      "✅ Generated embeddings for 50 notes\n"
     ]
    }
   ],
   "source": [
    "# Display first 3 embeddings for verification\n",
    "for n in notes[:3]:\n",
    "    print(f\"Claim ID: {n['claim_id']}, Note ID: {n['note_id']}\")\n",
    "    print(f\"Embedding vector length: {len(n['embedding'])}\")\n",
    "    print(f\"Embedding vector: {n['embedding'][:5]}...\\n\")\n",
    "\n",
    "print(f\"✅ Generated embeddings for {len(notes)} notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63044d05",
   "metadata": {},
   "source": [
    "## Insert Notes & Embeddings into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b1002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inserted 50 notes into PostgreSQL + pgvector\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import numpy as np\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"claims_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"admin\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insert query with ON CONFLICT to avoid duplicates\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO claim_notes (claim_id, note_id, file_path, note_text, embedding)\n",
    "VALUES %s\n",
    "ON CONFLICT (claim_id, note_id) DO NOTHING\n",
    "\"\"\"\n",
    "\n",
    "# Prepare data for insertion\n",
    "# Convert numpy embeddings to list for psycopg2\n",
    "data = [\n",
    "    (n[\"claim_id\"], n[\"note_id\"], n[\"file_path\"], n[\"text\"], n[\"embedding\"].tolist())\n",
    "    for n in notes\n",
    "]\n",
    "\n",
    "# Bulk insert using execute_values for efficiency\n",
    "extras.execute_values(cur, insert_query, data)\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\" Inserted {len(data)} notes into PostgreSQL + pgvector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac38583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>note_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>3001</td>\n",
       "      <td>claim_notes_txt\\claim_2001_note_3001.txt</td>\n",
       "      <td>Called the claimant at 708-1234 to confirm the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>3002</td>\n",
       "      <td>claim_notes_txt\\claim_2001_note_3002.txt</td>\n",
       "      <td>Spoke with the claimant at 708-1234 regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>3003</td>\n",
       "      <td>claim_notes_txt\\claim_2001_note_3003.txt</td>\n",
       "      <td>Completed an initial file review after receivi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>3004</td>\n",
       "      <td>claim_notes_txt\\claim_2001_note_3004.txt</td>\n",
       "      <td>To-do: confirm receipt of photographic evidenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>3005</td>\n",
       "      <td>claim_notes_txt\\claim_2001_note_3005.txt</td>\n",
       "      <td>Called the claimant at 708-1234 to discuss cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id note_id                                 file_path  \\\n",
       "0     2001    3001  claim_notes_txt\\claim_2001_note_3001.txt   \n",
       "1     2001    3002  claim_notes_txt\\claim_2001_note_3002.txt   \n",
       "2     2001    3003  claim_notes_txt\\claim_2001_note_3003.txt   \n",
       "3     2001    3004  claim_notes_txt\\claim_2001_note_3004.txt   \n",
       "4     2001    3005  claim_notes_txt\\claim_2001_note_3005.txt   \n",
       "\n",
       "                                           note_text  \n",
       "0  Called the claimant at 708-1234 to confirm the...  \n",
       "1  Spoke with the claimant at 708-1234 regarding ...  \n",
       "2  Completed an initial file review after receivi...  \n",
       "3  To-do: confirm receipt of photographic evidenc...  \n",
       "4  Called the claimant at 708-1234 to discuss cov...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "claim_to_fetch = \"2001\"\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"claims_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"admin\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Query notes for the claim\n",
    "cur.execute(\"\"\"\n",
    "SELECT claim_id, note_id, file_path, note_text\n",
    "FROM claim_notes\n",
    "WHERE claim_id = %s\n",
    "ORDER BY note_id\n",
    "\"\"\", (claim_to_fetch,))\n",
    "\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Convert to DataFrame for easy display\n",
    "df_notes = pd.DataFrame(rows, columns=[\"claim_id\", \"note_id\", \"file_path\", \"note_text\"])\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Display in notebook (first 5 notes)\n",
    "df_notes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a159d",
   "metadata": {},
   "source": [
    "# Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f6ec787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.10\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "print(psycopg.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33eb6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "#  Embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#  Postgres connection\n",
    "connection = \"postgresql+psycopg2://postgres:admin@localhost:5432/claims_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5043168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Vector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"claim_notes\",\n",
    "    connection=connection,\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e8b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 50 claim notes ingested into PGVector!\n"
     ]
    }
   ],
   "source": [
    "# Ingest claim notes\n",
    "notes_dir = \"claim_notes_txt\"  # folder containing claim_id_note_id.txt files\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(notes_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        claim_id, note_id = file.replace(\".txt\", \"\").split(\"_note_\")\n",
    "        file_path = os.path.join(notes_dir, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        metadata = {\"claim_id\": claim_id, \"note_id\": note_id, \"file_path\": file_path}\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "#  Add documents to PGVector\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "\n",
    "print(f\"✅ {len(documents)} claim notes ingested into PGVector!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04a66f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant notes:\n",
      "\n",
      "1. Claim ID: claim_2001 | Note ID: 3002\n",
      "File: claim_notes_txt\\claim_2001_note_3002.txt\n",
      "Snippet: Spoke with the claimant at 708-1234 regarding the extent of visible damages and locations affected. The claimant described interior water damage to two rooms and provided estimates verbally for tempor...\n",
      "\n",
      "2. Claim ID: claim_2001 | Note ID: 3001\n",
      "File: claim_notes_txt\\claim_2001_note_3001.txt\n",
      "Snippet: Called the claimant at 708-1234 to confirm the initial loss details and timeline. The claimant provided a thorough account of events leading to the loss and described immediate mitigation steps taken ...\n",
      "\n",
      "3. Claim ID: claim_2001 | Note ID: 3007\n",
      "File: claim_notes_txt\\claim_2001_note_3007.txt\n",
      "Snippet: Discussed potential injury reports with the claimant at 708-1234 following their statement of minor cuts sustained during the loss. I requested any medical documentation and informed claimant of the m...\n",
      "\n",
      "4. Claim ID: claim_2001 | Note ID: 3009\n",
      "File: claim_notes_txt\\claim_2001_note_3009.txt\n",
      "Snippet: Explained personal injury protection (PIP) considerations to the claimant at 708-1234 who inquired about immediate medical expense coverage. I outlined eligible expenses, documentation standards, and ...\n",
      "\n",
      "5. Claim ID: claim_2001 | Note ID: 3005\n",
      "File: claim_notes_txt\\claim_2001_note_3005.txt\n",
      "Snippet: Called the claimant at 708-1234 to discuss coverage specifics and deductible implications. The claimant had questions about replacement cost versus actual cash value and I explained relevant policy la...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: retrieve notes mentioning \"water damage\" for claim_2000\n",
    "query = \"water damage\"\n",
    "filter = {\"claim_id\": \"claim_2001\"}  # match exactly what is in cmetadata\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    filter=filter,\n",
    "    k=5  # top 5 matches\n",
    ")\n",
    "\n",
    "print(f\"Found {len(results)} relevant notes:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    meta = doc.metadata\n",
    "    print(f\"{i}. Claim ID: {meta['claim_id']} | Note ID: {meta['note_id']}\")\n",
    "    print(f\"File: {meta['file_path']}\")\n",
    "    print(f\"Snippet: {doc.page_content[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant notes:\n",
      "\n",
      "1. Claim ID: claim_2001 | Note ID: 3002\n",
      "File: claim_notes_txt\\claim_2001_note_3002.txt\n",
      "Snippet: Spoke with the claimant at 708-1234 regarding the extent of visible damages and locations affected. The claimant described interior water damage to two rooms and provided estimates verbally for tempor...\n",
      "\n",
      "2. Claim ID: claim_2004 | Note ID: 3032\n",
      "File: claim_notes_txt\\claim_2004_note_3032.txt\n",
      "Snippet: Spoke with claimant at 206-5678 regarding several areas of property damage and collected detailed descriptions of each affected area. The claimant described visible water staining, warped flooring, an...\n",
      "\n",
      "3. Claim ID: claim_2002 | Note ID: 3012\n",
      "File: claim_notes_txt\\claim_2002_note_3012.txt\n",
      "Snippet: Received initial photos from claimant at 312-3456 and performed a preliminary assessment; photos indicate localized siding loss and possible water intrusion. I documented visible damage and recommende...\n",
      "\n",
      "4. Claim ID: claim_2005 | Note ID: 3042\n",
      "File: claim_notes_txt\\claim_2005_note_3042.txt\n",
      "Snippet: Claimant provided an extensive photo set and I reviewed images showing interior and exterior damage patterns. I contacted the claimant at 415-6789 to walk through each photo and capture additional con...\n",
      "\n",
      "5. Claim ID: claim_2002 | Note ID: 3018\n",
      "File: claim_notes_txt\\claim_2002_note_3018.txt\n",
      "Snippet: Discussed potential third party responsibility for falling debris and liability implications with claimant at 312-3456. Claimant provided the landlord contact and approximate timeline when the debris ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example semantic-only query\n",
    "query = \"interior water damage in two rooms\"\n",
    "\n",
    "# Run semantic search without metadata filter\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=5  # top 5 results\n",
    ")\n",
    "\n",
    "print(f\"Found {len(results)} relevant notes:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    meta = doc.metadata\n",
    "    print(f\"{i}. Claim ID: {meta['claim_id']} | Note ID: {meta['note_id']}\")\n",
    "    print(f\"File: {meta['file_path']}\")\n",
    "    print(f\"Snippet: {doc.page_content[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e657469",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
